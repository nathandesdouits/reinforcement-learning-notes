{
 "metadata": {
  "name": "Chapter 3 - finite Markov Decision Processes"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Finite Markov Decision Processes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Definition of finite Markov Decision Processes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finite Markov decision processes are the set of problems that can be solved by Reinforcement Learning methods."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At each time step $t$, the agent is given a representation of the environment's state $S_t$, and must chose between actions $A_t \\in \\mathcal{A}(S_t)$.\n",
      "At the next time step, the agent receives a reward $R_{t+1}$ and a new state $S_{t+1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To select the action $A_t$, the agent uses a mapping from the states to probabilities to select each of the actions. This mapping is called a policy $\\pi$: $\\pi_t(a|s)$ is the probability of taking action $A_t = a$ if $S_t = s$ at time $t$ under the policy $pi$. The goal of reinforcement learning method is to find a policy that maximizes the cumulative sum of the rewards $G_t$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This cumulative sum can be unweighted for tasks that ends in a finite number of steps (episodic tasks), or weighted by terms which assure convergence for infinite-steps tasks. To avoid distinguishing between episodic and infinite tasks, we can describe episodic tasks as an infinite one, which ends with a terminal state that always return to itsels with reward 0 no matter what action is chosen."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The weighted sum is usually done by *discounting* the expected reward for each subsequent step by a factor $\\gamma$: $G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$. By doing this, we put more emphasis on short-term future rewards and less on long-term ones."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Markov property"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A state signal has the Markov property if at each step it contains all the relevant information necessary to make optimal decisions. More formally, and assuming finite number of states and reward values, we define the general general dynamics of the state signal by this probability distribution:\n",
      "\n",
      "\\begin{align*}\n",
      "Pr \\\\{ R_{t+1} = r, S_{t+1} = s' | S_0, A_0, R_1, \\ldots, S_{t-1},A_{t-1},R_t,S_t,A_t \\\\}\n",
      "\\end{align*}\n",
      "\n",
      "A state signal that has the Markov property has a dynamic that can be defined by just using $S_t$ and $A_t$:\n",
      "\n",
      "\\begin{align*}\n",
      " p(s', r|s,a) &= Pr \\\\{ R_{t+1} = r, S_{t+1} = s' | S_t,A_t \\\\}\n",
      "\\end{align*}\n",
      "\n",
      "Note that RL algorithms can tackle non Markov problem, as long as the state contains sufficient information to make reasonable choices of actions.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using that dynamic definition, one can compute the expected reward for state-action pairs $r(s,a)$ and state-action-next-state triplets $r(s,a,s')$, and the *state-transition probabilities* $p(s'|s,a)$:\n",
      "\n",
      "\\begin{align*}\n",
      "r(s,a) &= \\mathbb{E}[R_{t+1} | S_t=s,A_t=a] &= \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a) \\\\\\\n",
      "p(s'|s,a) &= Pr \\\\{ S_{t+1}=s' | S_t=s,A_t=a \\\\} &= \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a) \\\\\\\n",
      "r(s,a,s') &= \\mathbb{E}[R_{t+1} | S_t=s,A_t=a,S_{t+1}=s'] &= \\frac{\\sum_{r \\in \\mathcal{R}} r p(s',r|s,a)}{p(s'|s,a)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Value function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To chose the correct actions at each step, RL algorithms usually try to estimate the value of the state they are in and the expected value they should have when performing the next actions. This value depends on the policy $\\pi(a|s)$ that is used (the policy dictates the way the actions are chosen). Thus, we will note $v_\\pi(s)$ the value associated to a particular policy $\\pi$ in the state $s$. We can define $v_\\pi$ as:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\pi(s) = \\mathbb{E}_\\pi [ G_t|S_t = s ] = \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s \\right]\n",
      "\\end{align*}\n",
      "\n",
      "$v_\\pi(s)$ is the *state-value function for policy $\\pi$*. We can also define $q_\\pi(s,a)$, the *action-value* function for policy $\\pi$:\n",
      "\\begin{align*}\n",
      "q_\\pi(s,a) = \\mathbb{E}_\\pi [ G_t|S_t = s, A_t = a ] = \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right]\n",
      "\\end{align*}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By developing the equation defining $v_\\pi$, we can derive the *Bellman equation* for $v_\\pi$:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\pi(s) &= \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s \\right] \\\\\\\n",
      "&= \\mathbb{E}\\_\\pi \\left[ R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\middle| S_t = s \\right] \\\\\\\n",
      "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\mathbb{E} \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\middle| S_t = s \\right]  \\right) \\\\\\\n",
      "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma v_\\pi(s') \\right)\n",
      "\\end{align*}\n",
      "\n",
      "Where $\\pi(a|s)$ is the probability to chose action $a$ in state $s$ under policy $\\pi$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optimal value function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A policy $\\pi$ is better than or equal to a policy $\\pi'$ (noted $\\pi \\geq \\pi'$) if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s \\in \\mathcal{S}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An optimal policy $\\pi_\\*$ is one that is better than or equal to all other policies, and has a value $v_\\*(s) = \\max_\\pi v_\\pi(s)$ and optimal action-value function $q_\\*(s,a) = \\max_\\pi q_\\pi(s,a)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then derive the Bellman optimality equation using the fact that the expected value must equal the expected return for the best action for that state:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\*(s) &= \\max_a q_*(s,a) \\\\\\\n",
      "& ... \\\\\\\n",
      "v_\\*(s)&= \\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma v_\\*(S_{t+1}) \\middle| S_t = s, A_t = a \\right] \\\\\\\n",
      "&= \\max_a \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma v_\\*(s') \\right)\n",
      "\\end{align*}\n",
      "\n",
      "And for $q_*$:\n",
      "\n",
      "\\begin{align*}\n",
      "q_\\*(s,a) &= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} q_\\* \\left( S_{t+1},a' \\right) \\middle| S_t = s, A_t = a \\right] \\\\\\\n",
      "&= \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\max_{a'} q_\\*(s',a') \\right)\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercises"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Exercise 3.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.*"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. A robotic arm built on a table has 3 joints: one base joint located between the table and the arm that can rotate both in the plane of the table and in the plane of the arm, one middle joint that can rotate in the plane of the arm and one end joint between the arm and the \"hand\" that can rotate in the plane of the arm. The hand is a simple pincher that can be opened or closed. The goal for the arm is to take one object on the table and put it in a fixed basket near the arm. The states are the angles for each joint, the state of the hand (opened or closed), and the position of the object. The actions are the torque applied to each joint and to the hand. The reward is composed of several terms: one fixed, slightly negative term for each timestep the object is not in the basket, one negative term that increases in amplitude the farther the hand is from the object (but null when the object is in the basket), one negative term that increases in amplitude the farther the object is from the basket, and one greatly positive term when the object reaches the basket.\n",
      "\n",
      "2. An \"AI\" for the GameBoy game *Tetris*. The state can either be the pixel values for the current frame (non-MDP problem) or for the list of frames since the beginning of the game (MDP problem). The actions are the different possible \"states\" (on/off) of all GameBoy controls (up down left right A B start select). The goal is for the AI to have the highest score possible at the game over screen. The reward is the difference in score between the current frame and the one before.\n",
      "\n",
      "3. (Not sure if \"correct\") A human brain. The state is the neural inputs coming from the 5 senses and nerves from the whole body. The actions are the activity of the motor neurons. The goal is to survive first and foremost, and be as happy as possible. The reward is composed of several terms: one negative term with an amplitude proportional to the pain level, one positive term with an amplitude proportional to the level of happiness (i.e: level of endorphines and other \"feel-good\" neurotransmitters), and one slightly positive fixed term for each moment where the subject is alive."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}