{
 "metadata": {
  "name": "Chapter 3 - finite Markov Decision Processes"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Finite Markov Decision Processes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Notes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Definition of finite Markov Decision Processes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finite Markov decision processes are the set of problems that can be solved by Reinforcement Learning methods."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At each time step $t$, the agent is given a representation of the environment's state $S_t$, and must chose between actions $A_t \\in \\mathcal{A}(S_t)$.\n",
      "At the next time step, the agent receives a reward $R_{t+1}$ and a new state $S_{t+1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To select the action $A_t$, the agent uses a mapping from the states to probabilities to select each of the actions. This mapping is called a policy $\\pi$: $\\pi_t(a|s)$ is the probability of taking action $A_t = a$ if $S_t = s$ at time $t$ under the policy $pi$. The goal of reinforcement learning method is to find a policy that maximizes the cumulative sum of the rewards $G_t$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This cumulative sum can be unweighted for tasks that ends in a finite number of steps (episodic tasks), or weighted by terms which assure convergence for infinite-steps tasks. To avoid distinguishing between episodic and infinite tasks, we can describe episodic tasks as an infinite one, which ends with a terminal state that always return to itsels with reward 0 no matter what action is chosen."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The weighted sum is usually done by *discounting* the expected reward for each subsequent step by a factor $\\gamma$: $G_t = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$. By doing this, we put more emphasis on short-term future rewards and less on long-term ones."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Markov property"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A state signal has the Markov property if at each step it contains all the relevant information necessary to make optimal decisions. More formally, and assuming finite number of states and reward values, we define the general general dynamics of the state signal by this probability distribution:\n",
      "\n",
      "\\begin{align*}\n",
      "Pr \\\\{ R_{t+1} = r, S_{t+1} = s' | S_0, A_0, R_1, \\ldots, S_{t-1},A_{t-1},R_t,S_t,A_t \\\\}\n",
      "\\end{align*}\n",
      "\n",
      "A state signal that has the Markov property has a dynamic that can be defined by just using $S_t$ and $A_t$:\n",
      "\n",
      "\\begin{align*}\n",
      " p(s', r|s,a) &= Pr \\\\{ R_{t+1} = r, S_{t+1} = s' | S_t,A_t \\\\}\n",
      "\\end{align*}\n",
      "\n",
      "Note that RL algorithms can tackle non Markov problem, as long as the state contains sufficient information to make reasonable choices of actions.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using that dynamic definition, one can compute the expected reward for state-action pairs $r(s,a)$ and state-action-next-state triplets $r(s,a,s')$, and the *state-transition probabilities* $p(s'|s,a)$:\n",
      "\n",
      "\\begin{align*}\n",
      "r(s,a) &= \\mathbb{E}[R_{t+1} | S_t=s,A_t=a] &= \\sum_{r \\in \\mathcal{R}} r \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a) \\\\\\\n",
      "p(s'|s,a) &= Pr \\\\{ S_{t+1}=s' | S_t=s,A_t=a \\\\} &= \\sum_{s' \\in \\mathcal{S}} p(s',r|s,a) \\\\\\\n",
      "r(s,a,s') &= \\mathbb{E}[R_{t+1} | S_t=s,A_t=a,S_{t+1}=s'] &= \\frac{\\sum_{r \\in \\mathcal{R}} r p(s',r|s,a)}{p(s'|s,a)}\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Value function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To chose the correct actions at each step, RL algorithms usually try to estimate the value of the state they are in and the expected value they should have when performing the next actions. This value depends on the policy $\\pi(a|s)$ that is used (the policy dictates the way the actions are chosen). Thus, we will note $v_\\pi(s)$ the value associated to a particular policy $\\pi$ in the state $s$. We can define $v_\\pi$ as:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\pi(s) = \\mathbb{E}_\\pi [ G_t|S_t = s ] = \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s \\right]\n",
      "\\end{align*}\n",
      "\n",
      "$v_\\pi(s)$ is the *state-value function for policy $\\pi$*. We can also define $q_\\pi(s,a)$, the *action-value* function for policy $\\pi$:\n",
      "\\begin{align*}\n",
      "q_\\pi(s,a) = \\mathbb{E}_\\pi [ G_t|S_t = s, A_t = a ] = \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s, A_t = a \\right]\n",
      "\\end{align*}\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By developing the equation defining $v_\\pi$, we can derive the *Bellman equation* for $v_\\pi$:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\pi(s) &= \\mathbb{E}\\_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\middle| S_t = s \\right] \\\\\\\n",
      "&= \\mathbb{E}\\_\\pi \\left[ R_{t+1} + \\gamma \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\middle| S_t = s \\right] \\\\\\\n",
      "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\mathbb{E} \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+2} \\middle| S_t = s \\right]  \\right) \\\\\\\n",
      "&= \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma v_\\pi(s') \\right)\n",
      "\\end{align*}\n",
      "\n",
      "Where $\\pi(a|s)$ is the probability to chose action $a$ in state $s$ under policy $\\pi$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Optimal value function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A policy $\\pi$ is better than or equal to a policy $\\pi'$ (noted $\\pi \\geq \\pi'$) if $v_\\pi(s) \\geq v_{\\pi'}(s)$ for all $s \\in \\mathcal{S}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "An optimal policy $\\pi_\\*$ is one that is better than or equal to all other policies, and has a value $v_\\*(s) = \\max_\\pi v_\\pi(s)$ and optimal action-value function $q_\\*(s,a) = \\max_\\pi q_\\pi(s,a)$ for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can then derive the Bellman optimality equation using the fact that the expected value must equal the expected return for the best action for that state:\n",
      "\n",
      "\\begin{align*}\n",
      "v_\\*(s) &= \\max_a q_*(s,a) \\\\\\\n",
      "& ... \\\\\\\n",
      "v_\\*(s)&= \\max_a \\mathbb{E} \\left[ R_{t+1} + \\gamma v_\\*(S_{t+1}) \\middle| S_t = s, A_t = a \\right] \\\\\\\n",
      "&= \\max_a \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma v_\\*(s') \\right)\n",
      "\\end{align*}\n",
      "\n",
      "And for $q_*$:\n",
      "\n",
      "\\begin{align*}\n",
      "q_\\*(s,a) &= \\mathbb{E} \\left[ R_{t+1} + \\gamma \\max_{a'} q_\\* \\left( S_{t+1},a' \\right) \\middle| S_t = s, A_t = a \\right] \\\\\\\n",
      "&= \\sum_{s',r} p(s',r|s,a) \\left( r + \\gamma \\max_{a'} q_\\*(s',a') \\right)\n",
      "\\end{align*}"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercises"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Exercise 3.1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*Devise three example tasks of your own that fit into the reinforcement learning framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples.*"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}